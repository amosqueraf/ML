Árboles de decisión
Los árboles de decisión son algoritmos de clasificación supervisada (existe una variable objetivo predefinida), que facilitan la comprensión del modelo. El árbol en sí mismo, al ser obtenido, determina una regla de decisión.  En un árbol de decisión se divide la población o muestra en conjuntos homegéneos basados en la variable de entrada más significativa. Esta técnica permite: Segmentación: establecer que grupos son importantes para clasificar un cierto ítem.  Clasificación: asignar ítems a uno de los grupos en que está particionada una población.  Predicción: establecer reglas para hacer predicciones de ciertos eventos.  Reducción de la dimensión de los datos: Identificar que datos son los importantes para hacer modelos de un fenómeno.  Identificación-interrelación: identificar que variables y relaciones son importantes para ciertos grupos identificados a partir de analizar los datos.  Recodificación: discretizar variables o establecer criterios cualitativos perdiendo la menor cantidad posible de información relevante.
Un árbol de decisión está formado por un conjunto de nodos de decisión (interiores) y de nodos-respuesta (hojas): Un nodo de decisión está asociado a uno de los atributos y tiene 2 o más ramas que salen de él, cada una de ellas representando los posibles valores que puede tomar el atributo asociado. De alguna forma, un nodo de decisión es como una pregunta que se le hace al ejemplo analizado, y dependiendo de la respuesta que dé, el flujo tomará una de las ramas salientes. Y un nodo-respuesta está asociado a la clasificación que se quiere proporcionar, y devuelve la decisión del árbol con respecto al ejemplo de entrada.
Para obtener el árbol óptimo y valorar cada subdivisión entre todos los árboles posibles y conseguir el nodo raíz y los subsiguientes, el algoritmo deberá medir de alguna manera las predicciones logradas y valorarlas para comparar de entre todas y obtener la mejor. Para medir y valorar, utiliza diversas funciones, siendo las más conocidas y usadas los “Indice gini” y “Ganancia de información” que utiliza la denominada “entropía “. La división de nodos continuará hasta que lleguemos a la profundidad máxima posible del árbol ó se limiten los nodos a una cantidad mínima de muestras en cada hoja. A continuación, describiremos muy brevemente cada una de las estrategias nombradas:
Indice Gini: Se utiliza para atributos con valores continuos (precio de una casa). Esta función de coste mide el “grado de impureza” de los nodos, es decir, cuán desordenados o mezclados quedan los nodos una vez divididos. Deberemos minimizar ese GINI index.
Ganancia de información: Se utiliza para atributos categóricos (cómo en hombre/mujer). Este criterio intenta estimar la información que aporta cada atributo basado en la “teoría de la información “. Para medir la aleatoriedad de incertidumbre de un valor aleatorio de una variable “X” se define la Entropia.
Al obtener la medida de entropía de cada atributo, podemos calcular la ganancia de información del árbol. Deberemos maximizar esa ganancia.
Sin embargo, los árboles de decisión tienen problemas de sobreajuste u “overfitting”, ya que se pueden realizar muchos de ellos y obtener un resultado para cada uno. Para evitar sobreajustar un árbol se utilizan el método de Random forest en el cual, cada uno de los árboles toma sus decisiones de clasificación con base en diferentes variables. Los modelos Random Forest hacen crecer árboles mucho más profundos que los árboles simples.
Según Hernández (2015) en su blog Herramientas para la Toma de Decisiones, existen diferentes tipos de árboles de decisión, que son catalogados según la situación y el resultado deseado, él los cataloga en los siguientes tipos:
1.	Árbol de clasificación o binario: se usa cuando hay varias alternativas que se han calculado anteriormente para obtener resultados más predecibles, para hacer uso de estos, hay que trazar esquemas binarios y proyectar las diferentes variables o ramas del árbol, gracias a las probabilidades de estas se puede predecir un poco el resultado. Es usado en probabilidad, estadística y minería de datos.
2.	Árbol de regresión: este tipo de árbol ayuda a determinar un único resultado ya que se tiene la información necesaria para identificar una “ruta” óptima. Cuando se construye este árbol se divide la información en secciones o subgrupos. Este tipo de árbol es muy usado en bienes raíces.
3.	Árbol de mejora: es usado cuando se desea tener un resultado más preciso; el árbol se construye, luego se toma una variable, esta se calcula y se estructura para reducir la incertidumbre y los errores a la hora de tomar decisiones. Este árbol es usado en contabilidad y matemática.
4.	Árboles mixtos entre regresión y clasificación: es usado para prever un resultado con variables impredecibles, generalmente se usan indicadores que muestren lo que ya ha sucedido en un pasado (Sesgando un poco el resultado), Se usa generalmente en ciencia.
5.	Arboles de binomiales y trinomiales: son árboles donde cada rama es en sí un modelo binomial su función es recrear el modelo binomial varias veces en el tiempo suponiendo que el precio o costo de un elemento sube o baja con el tiempo. Estos árboles con muy usados en la aplicación de opciones reales.
